{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune GPT2 to generate positive reviews\n",
    "> Optimise GPT2 to produce positive IMDB movie reviews using a BERT sentiment classifier as a reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
    "<p style=\"text-align: center;\"> <b>Figure:</b> Experiment setup to tune GPT2. The yellow arrows are outside the scope of this notebook, but the trained models are available through Hugging Face. </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this notebook we fine-tune GPT2 (small) to generate positive movie reviews based on the IMDB dataset. The model gets the start of a real review and is tasked to produce positive continuations. To reward positive continuations we use a BERT classifier to analyse the sentiment of the produced sentences and use the classifier's outputs as rewards signals for PPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"bigscience/bloomz-560m\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjack-smith\u001b[0m (\u001b[33mjack-smith-Intellium AI\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacks/AEGIS/demos/wandb/run-20240701_132531-4bzfhfym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos/runs/4bzfhfym' target=\"_blank\">young-paper-5</a></strong> to <a href='https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos/runs/4bzfhfym' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos/runs/4bzfhfym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jack-smith-Intellium%20AI/AEGIS-demos/runs/4bzfhfym?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc93ad47df0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we load a GPT2 model called `gpt2_imdb`. This model was additionally fine-tuned on the IMDB dataset for 1 epoch with the huggingface [script](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) (no special settings). The other parameters are mostly taken from the original paper [\"Fine-Tuning Language Models from Human Preferences\"](\n",
    "https://arxiv.org/pdf/1909.08593.pdf). This model as well as the BERT model is available in the Huggingface model zoo [here](https://huggingface.co/models). The following code should automatically download the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB dataset\n",
    "The IMDB dataset contains 50k movie review annotated with \"positive\"/\"negative\" feedback indicating the sentiment.  We load the IMDB dataset into a DataFrame and filter for comments that are at least 200 characters. Then we tokenize each text and cut it to random size with the `LengthSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GPT2 language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the GPT2 model with a value head and the tokenizer. We load the model twice; the first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'BloomAttention'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n\u001b[1;32m      3\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBloomAttention\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLMWithValueHead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLMWithValueHead\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/models/modeling_base.py:233\u001b[0m, in \u001b[0;36mPreTrainedModelWrapper.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m is_loaded_in_8bit \u001b[38;5;129;01mor\u001b[39;00m is_loaded_in_4bit:\n\u001b[1;32m    229\u001b[0m                 pretrained_model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(\n\u001b[1;32m    230\u001b[0m                     pretrained_model,\n\u001b[1;32m    231\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpeft_quantization_kwargs,\n\u001b[1;32m    232\u001b[0m                 )\n\u001b[0;32m--> 233\u001b[0m             pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft adapter initialised\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pretrained_model_name_or_path, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msupported_pretrained_model_architectures):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/mapping.py:149\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    148\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/peft_model.py:1395\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/peft_model.py:138\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:166\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:375\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'BloomAttention'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from peft.tuners.lora import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(task_type=\"CAUSAL_LM\", r=4, lora_dropout=0.05, target_modules=[\"BloomAttention\"])\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, peft_config=peft_config)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def print_model_structure(model, depth=0, max_depth=None):\n",
    "    if max_depth is not None and depth > max_depth:\n",
    "        return\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        print('  ' * depth + f'|-- {name} ({type(module).__name__})')\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            print('  ' * (depth + 1) + f'|-- in_features: {module.in_features}, out_features: {module.out_features}')\n",
    "        \n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            print('  ' * (depth + 1) + f'|-- in_channels: {module.in_channels}, out_channels: {module.out_channels}, kernel_size: {module.kernel_size}')\n",
    "        \n",
    "        if hasattr(module, 'weight') and isinstance(module.weight, nn.Parameter):\n",
    "            print('  ' * (depth + 1) + f'|-- weight shape: {module.weight.shape}')\n",
    "        \n",
    "        if hasattr(module, 'bias') and isinstance(module.bias, nn.Parameter):\n",
    "            print('  ' * (depth + 1) + f'|-- bias shape: {module.bias.shape}')\n",
    "        \n",
    "        print_model_structure(module, depth + 1, max_depth)\n",
    "\n",
    "def find_trainable_modules(model):\n",
    "    trainable_modules = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            module_name = name.split('.')[0]  # Get the top-level module name\n",
    "            if module_name not in trainable_modules:\n",
    "                trainable_modules.append(module_name)\n",
    "    return trainable_modules\n",
    "\n",
    "# Usage example\n",
    "# model = YourModelHere()\n",
    "# print_model_structure(model, max_depth=None)  # Set max_depth to limit the depth of printed structure\n",
    "# trainable_modules = find_trainable_modules(model)\n",
    "# print(\"Trainable modules:\", trainable_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-- pretrained_model (PeftModelForCausalLM)\n",
      "  |-- base_model (LoraModel)\n",
      "    |-- model (BloomForCausalLM)\n",
      "      |-- transformer (BloomModel)\n",
      "        |-- word_embeddings (Embedding)\n",
      "          |-- weight shape: torch.Size([250880, 1024])\n",
      "        |-- word_embeddings_layernorm (LayerNorm)\n",
      "          |-- weight shape: torch.Size([1024])\n",
      "          |-- bias shape: torch.Size([1024])\n",
      "        |-- h (ModuleList)\n",
      "          |-- 0 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 1 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 2 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 3 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 4 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 5 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 6 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 7 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 8 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 9 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 10 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 11 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 12 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 13 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 14 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 15 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 16 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 17 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 18 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 19 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 20 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 21 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 22 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "          |-- 23 (BloomBlock)\n",
      "            |-- input_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- self_attention (BloomAttention)\n",
      "              |-- query_key_value (Linear)\n",
      "                |-- weight shape: torch.Size([3072, 1024])\n",
      "                |-- bias shape: torch.Size([3072])\n",
      "                |-- base_layer (Linear)\n",
      "                  |-- in_features: 1024, out_features: 3072\n",
      "                  |-- weight shape: torch.Size([3072, 1024])\n",
      "                  |-- bias shape: torch.Size([3072])\n",
      "                |-- lora_dropout (ModuleDict)\n",
      "                  |-- default (Dropout)\n",
      "                |-- lora_A (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 1024, out_features: 4\n",
      "                    |-- weight shape: torch.Size([4, 1024])\n",
      "                |-- lora_B (ModuleDict)\n",
      "                  |-- default (Linear)\n",
      "                    |-- in_features: 4, out_features: 3072\n",
      "                    |-- weight shape: torch.Size([3072, 4])\n",
      "                |-- lora_embedding_A (ParameterDict)\n",
      "                |-- lora_embedding_B (ParameterDict)\n",
      "              |-- dense (Linear)\n",
      "                |-- in_features: 1024, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 1024])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "              |-- attention_dropout (Dropout)\n",
      "            |-- post_attention_layernorm (LayerNorm)\n",
      "              |-- weight shape: torch.Size([1024])\n",
      "              |-- bias shape: torch.Size([1024])\n",
      "            |-- mlp (BloomMLP)\n",
      "              |-- dense_h_to_4h (Linear)\n",
      "                |-- in_features: 1024, out_features: 4096\n",
      "                |-- weight shape: torch.Size([4096, 1024])\n",
      "                |-- bias shape: torch.Size([4096])\n",
      "              |-- gelu_impl (BloomGelu)\n",
      "              |-- dense_4h_to_h (Linear)\n",
      "                |-- in_features: 4096, out_features: 1024\n",
      "                |-- weight shape: torch.Size([1024, 4096])\n",
      "                |-- bias shape: torch.Size([1024])\n",
      "        |-- ln_f (LayerNorm)\n",
      "          |-- weight shape: torch.Size([1024])\n",
      "          |-- bias shape: torch.Size([1024])\n",
      "      |-- lm_head (Linear)\n",
      "        |-- in_features: 1024, out_features: 250880\n",
      "        |-- weight shape: torch.Size([250880, 1024])\n",
      "|-- v_head (ValueHead)\n",
      "  |-- dropout (Dropout)\n",
      "  |-- summary (Linear)\n",
      "    |-- in_features: 1024, out_features: 1\n",
      "    |-- weight shape: torch.Size([1, 1024])\n",
      "    |-- bias shape: torch.Size([1])\n",
      "  |-- flatten (Flatten)\n"
     ]
    }
   ],
   "source": [
    "print_model_structure(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:z8qk3eg4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-planet-7</strong> at: <a href='https://wandb.ai/jack-smith-Intellium%20AI/trl/runs/z8qk3eg4' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/trl/runs/z8qk3eg4</a><br/> View project at: <a href='https://wandb.ai/jack-smith-Intellium%20AI/trl' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/trl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_130742-z8qk3eg4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:z8qk3eg4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacks/AEGIS/demos/wandb/run-20240701_131535-81hoixz3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jack-smith-Intellium%20AI/trl/runs/81hoixz3' target=\"_blank\">proud-shadow-8</a></strong> to <a href='https://wandb.ai/jack-smith-Intellium%20AI/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jack-smith-Intellium%20AI/trl' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jack-smith-Intellium%20AI/trl/runs/81hoixz3' target=\"_blank\">https://wandb.ai/jack-smith-Intellium%20AI/trl/runs/81hoixz3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_hub_mixin_config': PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with='wandb', task_name=None, model_name='bigscience/bloomz-560m', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=1, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, gradient_checkpointing=False, is_encoder_decoder=False, is_peft_model=True, backward_batch_size=128, global_backward_batch_size=128, global_batch_size=128),\n",
       " 'config': PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with='wandb', task_name=None, model_name='bigscience/bloomz-560m', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=1, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, gradient_checkpointing=False, is_encoder_decoder=False, is_peft_model=True, backward_batch_size=128, global_backward_batch_size=128, global_batch_size=128),\n",
       " 'accelerator': <accelerate.accelerator.Accelerator at 0x7f446c6a8430>,\n",
       " 'model': AutoModelForCausalLMWithValueHead(\n",
       "   (pretrained_model): PeftModelForCausalLM(\n",
       "     (base_model): LoraModel(\n",
       "       (model): BloomForCausalLM(\n",
       "         (transformer): BloomModel(\n",
       "           (word_embeddings): Embedding(250880, 1024)\n",
       "           (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (h): ModuleList(\n",
       "             (0-23): 24 x BloomBlock(\n",
       "               (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (self_attention): BloomAttention(\n",
       "                 (query_key_value): lora.Linear(\n",
       "                   (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                   (lora_dropout): ModuleDict(\n",
       "                     (default): Dropout(p=0.05, inplace=False)\n",
       "                   )\n",
       "                   (lora_A): ModuleDict(\n",
       "                     (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                   )\n",
       "                   (lora_B): ModuleDict(\n",
       "                     (default): Linear(in_features=4, out_features=3072, bias=False)\n",
       "                   )\n",
       "                   (lora_embedding_A): ParameterDict()\n",
       "                   (lora_embedding_B): ParameterDict()\n",
       "                 )\n",
       "                 (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                 (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "               (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): BloomMLP(\n",
       "                 (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                 (gelu_impl): BloomGelu()\n",
       "                 (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (v_head): ValueHead(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (summary): Linear(in_features=1024, out_features=1, bias=True)\n",
       "     (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       " ),\n",
       " 'model_params': <filter at 0x7f446c6a84f0>,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_peft_model': True,\n",
       " 'is_using_text_environment': False,\n",
       " 'ref_model': AutoModelForCausalLMWithValueHead(\n",
       "   (pretrained_model): BloomForCausalLM(\n",
       "     (transformer): BloomModel(\n",
       "       (word_embeddings): Embedding(250880, 1024)\n",
       "       (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (h): ModuleList(\n",
       "         (0-23): 24 x BloomBlock(\n",
       "           (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (self_attention): BloomAttention(\n",
       "             (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "             (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "           )\n",
       "           (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): BloomMLP(\n",
       "             (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (gelu_impl): BloomGelu()\n",
       "             (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "   )\n",
       "   (v_head): ValueHead(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (summary): Linear(in_features=1024, out_features=1, bias=True)\n",
       "     (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       " ),\n",
       " 'optional_peft_ctx': <bound method PeftModel.disable_adapter of PeftModelForCausalLM(\n",
       "   (base_model): LoraModel(\n",
       "     (model): BloomForCausalLM(\n",
       "       (transformer): BloomModel(\n",
       "         (word_embeddings): Embedding(250880, 1024)\n",
       "         (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (h): ModuleList(\n",
       "           (0-23): 24 x BloomBlock(\n",
       "             (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (self_attention): BloomAttention(\n",
       "               (query_key_value): lora.Linear(\n",
       "                 (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                 (lora_dropout): ModuleDict(\n",
       "                   (default): Dropout(p=0.05, inplace=False)\n",
       "                 )\n",
       "                 (lora_A): ModuleDict(\n",
       "                   (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                 )\n",
       "                 (lora_B): ModuleDict(\n",
       "                   (default): Linear(in_features=4, out_features=3072, bias=False)\n",
       "                 )\n",
       "                 (lora_embedding_A): ParameterDict()\n",
       "                 (lora_embedding_B): ParameterDict()\n",
       "               )\n",
       "               (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "             (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (mlp): BloomMLP(\n",
       "               (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "               (gelu_impl): BloomGelu()\n",
       "               (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "     )\n",
       "   )\n",
       " )>,\n",
       " 'tokenizer': BloomTokenizerFast(name_or_path='bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " },\n",
       " 'dataset': Dataset({\n",
       "     features: ['review', 'label', 'input_ids', 'query'],\n",
       "     num_rows: 24895\n",
       " }),\n",
       " '_signature_columns': ['input_ids',\n",
       "  'past_key_values',\n",
       "  'attention_mask',\n",
       "  'kwargs',\n",
       "  'label',\n",
       "  'query',\n",
       "  'response'],\n",
       " 'dataloader': <accelerate.data_loader.DataLoaderShard at 0x7f446c3ec400>,\n",
       " 'data_collator': DataCollatorForLanguageModeling(tokenizer=BloomTokenizerFast(name_or_path='bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),\n",
       " 'optimizer': AcceleratedOptimizer (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 1.41e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'lr_scheduler': None,\n",
       " 'kl_ctl': <trl.trainer.utils.AdaptiveKLController at 0x7f446c3ed9c0>,\n",
       " 'is_distributed': False,\n",
       " 'current_step': 0,\n",
       " 'current_device': device(type='cuda'),\n",
       " 'running': <trl.trainer.utils.RunningMoments at 0x7f446c3a6a10>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_trainer.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT classifier\n",
    "We load a BERT classifier fine-tuned on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacks/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 2.3350486755371094},\n",
       "  {'label': 'POSITIVE', 'score': -2.726576566696167}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': -2.294790029525757},\n",
       "  {'label': 'POSITIVE', 'score': 2.557040214538574}]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation settings\n",
    "For the response generation we just use sampling and make sure top-k and nucleus sampling are turned off as well as a minimal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the query responses from the policy network (GPT-2)\n",
    "2. Get sentiments for query/responses from BERT\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet\n",
    "\n",
    "**Training time**\n",
    "\n",
    "This step takes **~2h** on a V100 GPU with the above specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:17, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.63 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(output[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m ppo_trainer\u001b[38;5;241m.\u001b[39mlog_stats(stats, batch, rewards)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:817\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries, responses, scores, response_masks)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    815\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m {k: mini_batch_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m model_inputs_names}\n\u001b[0;32m--> 817\u001b[0m     logprobs, logits, vpreds, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatched_forward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqueries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmini_batch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_minibatch(\n\u001b[1;32m    825\u001b[0m         mini_batch_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    826\u001b[0m         mini_batch_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m         mini_batch_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     all_stats\u001b[38;5;241m.\u001b[39mappend(train_stats)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1013\u001b[0m, in \u001b[0;36mPPOTrainer.batched_forward_pass\u001b[0;34m(self, model, queries, responses, model_inputs, return_logits, response_masks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m     response_masks_batch \u001b[38;5;241m=\u001b[39m response_masks[i \u001b[38;5;241m*\u001b[39m fbs : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m fbs]\n\u001b[0;32m-> 1013\u001b[0m logits, _, values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m   1016\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/models/modeling_value_head.py:171\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model\u001b[38;5;241m.\u001b[39mactive_peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREFIX_TUNING\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m base_model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m base_model_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    178\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m base_model_output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:861\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    848\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    849\u001b[0m     input_ids,\n\u001b[1;32m    850\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    858\u001b[0m )\n\u001b[1;32m    859\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 861\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.63 GiB. GPU "
     ]
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress\n",
    "If you are tracking the training progress with Weights&Biases you should see a plot similar to the one below. Check out the interactive sample report on wandb.ai: [link](https://app.wandb.ai/huggingface/trl-showcase/runs/1jtvxb1m/).\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_tuning_progress.png' width='800'>\n",
    "<p style=\"text-align: center;\"> <b>Figure:</b> Reward mean and distribution evolution during training. </p>\n",
    "</div>\n",
    "\n",
    "One can observe how the model starts to generate more positive outputs after a few optimisation steps.\n",
    "\n",
    "> Note: Investigating the KL-divergence will probably show that at this point the model has not converged to the target KL-divergence, yet. To get there would require longer training or starting with a higher initial coefficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inspection\n",
    "Let's inspect some examples from the IMDB dataset. We can use `model_ref` to compare the tuned model `model` against the model before optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences we observe a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Finally, we save the model and push it to the Hugging Face for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
