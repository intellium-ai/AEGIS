{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft.tuners.prefix_tuning import PrefixTuningConfig\n",
    "from peft.mapping import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain the moon landing to a 6 year old in a few sentences.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
    "\n",
    "dataset[\"query\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"bigscience/bloomz-560m\",\n",
    "    learning_rate=1.41e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from peft.tuners.lora import LoraConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain the moon landing to a 6 year old in a few sentences.',\n",
       " 'input_ids': [72535,\n",
       "  789,\n",
       "  368,\n",
       "  63270,\n",
       "  102205,\n",
       "  427,\n",
       "  267,\n",
       "  1231,\n",
       "  5559,\n",
       "  10735,\n",
       "  361,\n",
       "  267,\n",
       "  12442,\n",
       "  93150,\n",
       "  17]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer: PPOTrainer = PPOTrainer( #type: ignore\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Is the following sentence positive or negative: I dislike you\\n Negative</s>'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ppo_trainer.generate(torch.tensor(tokenizer.encode(\"<s>Is the following sentence positive or negative: I dislike you\\n\")).to(\"cuda:0\"), **generation_kwargs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:03<?, ?it/s] ?it/s]\n",
      "epoch:   0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch size (128) does not match number of examples - but got 1 for: queries",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         stats \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mresponse_tensors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         ppo_trainer\u001b[38;5;241m.\u001b[39mlog_stats(stats, batch, rewards)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#### Save model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:672\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[0;34m(self, queries, responses, scores, response_masks)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03mRun a PPO optimisation step given a list of queries, model responses, and rewards.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    `dict[str, Any]`: A summary of the training statistics\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    670\u001b[0m bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m--> 672\u001b[0m queries, responses, scores, response_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_safety_checker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_masks\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scores, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device)\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_score_scaling:\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# Score scaling\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aegis-gJ3S_XbT-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:627\u001b[0m, in \u001b[0;36mPPOTrainer._step_safety_checker\u001b[0;34m(self, batch_size, queries, responses, scores, masks)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElements in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be tensors - got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor_list) \u001b[38;5;241m!=\u001b[39m batch_size:\n\u001b[0;32m--> 627\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match number of examples - but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tensor_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m         )\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# add queries, scores and responses on the correct device\u001b[39;00m\n\u001b[1;32m    632\u001b[0m queries \u001b[38;5;241m=\u001b[39m [tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_device) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m queries]\n",
      "\u001b[0;31mValueError\u001b[0m: Batch size (128) does not match number of examples - but got 1 for: queries"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(dataset):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "        #### Get response from SFTModel\n",
    "        response_tensors: torch.Tensor = ppo_trainer.generate(torch.tensor(query_tensors).to(\"cuda:0\"), **generation_kwargs) #type: ignore\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "        #### Compute reward score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = reward_model(texts)\n",
    "        rewards = [torch.tensor(output[\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step([torch.tensor(query_tensors)], [response_tensors], [rewards])\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aegis-gJ3S_XbT-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
